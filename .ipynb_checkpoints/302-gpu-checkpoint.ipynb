{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8d1e100-d867-4a48-bd99-308597f7cf68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 24 02:44:30 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 537.58       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 Ti     On  | 00000000:2B:00.0  On |                  N/A |\n",
      "|  0%   46C    P5              20W / 160W |   1531MiB /  8188MiB |     40%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A        23      G   /Xwayland                                 N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14a3ea61-2068-4c57-aa8d-6a1053777267",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), device(type='cuda'), device(type='cuda', index=1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "torch.device('cpu'), torch.device('cuda'), torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "057c2aaf-1f6d-49fb-87a8-06bd53598d1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a62acf15-96b7-487e-ac37-98d1de10c374",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0),\n",
       " device(type='cpu'),\n",
       " [device(type='cuda', index=0)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_gpu(i=0):\n",
    "    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def try_all_gpus():\n",
    "    \"\"\"返回所有可用的GPU，如果没有GPU，则返回[cpu(),]\"\"\"\n",
    "    devices = [torch.device(f'cuda:{i}')\n",
    "                for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]\n",
    "\n",
    "try_gpu(), try_gpu(10), try_all_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35777f32-452d-494a-a909-1b91ef6364f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f99fd52-074e-4a9e-86d6-4041c97f113d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones(2, 3, device=try_gpu())\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17239c15-f2c8-48a7-ad03-d05821360f5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.985057592391968, 0.30353307723999023)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 当矩阵较小，cpu的速度会比gpu要快，但是当矩阵维度很大时，使用gpu更快\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Set the device to CPU and GPU\n",
    "device_cpu = torch.device(\"cpu\")\n",
    "device_gpu = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generate two 100x100 matrices of random numbers for both CPU and GPU\n",
    "matrix1_cpu = torch.rand(1000, 1000, device=device_cpu)\n",
    "matrix2_cpu = torch.rand(1000, 1000, device=device_cpu)\n",
    "\n",
    "matrix1_gpu = torch.rand(1000, 1000, device=device_gpu)\n",
    "matrix2_gpu = torch.rand(1000, 1000, device=device_gpu)\n",
    "\n",
    "# Measure the time taken to perform 1000 matrix multiplications on CPU\n",
    "start_time_cpu = time.time()\n",
    "for _ in range(1000):\n",
    "    result_cpu = torch.matmul(matrix1_cpu, matrix2_cpu)\n",
    "end_time_cpu = time.time()\n",
    "total_time_cpu = end_time_cpu - start_time_cpu\n",
    "\n",
    "# Measure the time taken to perform 1000 matrix multiplications on GPU\n",
    "torch.cuda.synchronize()  # Ensure CUDA operations are synchronized\n",
    "start_time_gpu = time.time()\n",
    "for _ in range(1000):\n",
    "    result_gpu = torch.matmul(matrix1_gpu, matrix2_gpu)\n",
    "torch.cuda.synchronize()  # Ensure CUDA operations are synchronized before measuring the end time\n",
    "end_time_gpu = time.time()\n",
    "total_time_gpu = end_time_gpu - start_time_gpu\n",
    "\n",
    "total_time_cpu, total_time_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a37728-31dd-4e88-b146-d8980afd065d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
